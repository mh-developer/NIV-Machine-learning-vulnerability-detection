import io
import subprocess

from pydriller import Repository


class W2VPythonTokenize:
    def __init__(self):
        self.tokenizer_result = None
        self.python_data = ""
        self.mode = "withString"
        self.count = 0
        self.total_count = 0
        self.comment = 0
        self.part = 0

    def default_tokenizer(self):
        p = subprocess.Popen(["python", "-m", "tokenize", "w2v/pythontraining.txt"], stdout=subprocess.PIPE)
        out, err = p.communicate()

        self.tokenizer_result = io.StringIO(str(out))

    def python_tokenize(self):
        for line in self.tokenizer_result:
            self.total_count += 1
            self.count += 1
            if self.total_count % 1000 == 0:
                print(self.total_count)
            position1 = line.find(":") + 1
            position2 = line.find("'")
            position3 = line[position2 + 1:].find("'")

            cat = line[position1:position2]
            content = line[position2 + 1:-2]

            if ('"""' in line) or ("COMMENT" in cat):
                self.comment += 1
                continue

            if ("NL" in cat) or ("NEWLINE" in cat):
                self.python_data = f"{self.python_data}\n"
            elif "INDENT" in cat:
                for x in range(content.count('t')):
                    self.python_data = f"{self.python_data}  "
            else:
                self.python_data = f"{self.python_data} {content}"

            # split into several files to reduce the computational load
            if self.count > 1000000:
                print("saving part " + str(self.part) + " (" + self.mode + ") " + str(self.total_count))
                with open(f'w2v/pythontraining_{self.mode}_{str(self.part)}', 'w', encoding="utf8") as outfile:
                    outfile.write(self.python_data)
                self.python_data = ""
                self.part += 1
                self.count = 0

        with open(f'w2v/pythontraining_{self.mode}_{str(self.part)}', 'w', encoding="utf8") as outfile:
            outfile.write(self.python_data)

    def collect_data_for_build_python_dictionary_corpus(self):
        repos = [
            "https://github.com/numpy/numpy",
            "https://github.com/django/django",
            "https://github.com/scikit-learn/scikit-learn",
            "https://github.com/tensorflow/tensorflow",
            "https://github.com/keras-team/keras",
            "https://github.com/ansible/ansible",
            "https://github.com/TheAlgorithms/Python",
            "https://github.com/pallets/flask",
            "https://github.com/ytdl-org/youtube-dl",
            "https://github.com/pandas-dev/pandas",
            "https://github.com/scrapy/scrapy",
            "https://github.com/kennethreitz/requests",
            "https://github.com/home-assistant/home-assistant",
            "https://github.com/ageitgey/face_recognition",
            "https://github.com/emesik/mamona",
            "https://github.com/progrium/notify-io",
            "https://github.com/phoenix2/phoenix",
            "https://github.com/odoo/odoo",
            "https://github.com/ageitgey/face_recognition",
            "https://github.com/psf/requests",
            "https://github.com/deepfakes/faceswap",
            "https://github.com/XX-net/XX-Net",
            "https://github.com/tornadoweb/tornado",
            "https://github.com/saltstack/salt",
            "https://github.com/matplotlib/matplotlib",
            "https://github.com/celery/celery",
            "https://github.com/binux/pyspider",
            "https://github.com/miguelgrinberg/flasky",
            "https://github.com/sqlmapproject/sqlmap",
            "https://github.com/zulip/zulip",
            "https://github.com/scipy/scipy",
            "https://github.com/bokeh/bokeh",
            "https://github.com/docker/compose",
            "https://github.com/getsentry/sentry",
            "https://github.com/timgrossmann/InstaPy",
            "https://github.com/divio/django-cms",
            "https://github.com/boto/boto"
        ]

        python_training = ""

        for repo in repos:
            print(repo)
            files = []
            for commit in Repository(repo).traverse_commits():
                for m in commit.modified_files:
                    filename = m.new_path
                    if filename is not None and ".py" in filename and not filename in files:
                        code = m.source_code
                        if code is not None:
                            python_training = f"{python_training}\n\n{code}"
                            files.append(filename)

            with open('w2v/pythontraining.txt', 'a', encoding="utf8") as outfile:
                outfile.write(python_training)
                python_training = ""

        self.remove_broken_code()

    def remove_broken_code(self):
        with open("w2v/pythontraining.txt", "r", encoding="utf8") as python_train_data:
            contents = python_train_data.read()
            contents = contents.replace('\t', '    ')

            if 'PositiveSmallIntegerField(\n                choices' in contents:
                pos = contents.find('PositiveSmallIntegerField(\n                choices')
                contents = contents[:pos - 198] + contents[pos + 178:]

            if "            raise ImportError,self.__dict__.get('_ppimport_exc_info')[1]" in contents:
                pos = contents.find("            raise ImportError,self.__dict__.get('_ppimport_exc_info')[1]")
                length = len("            raise ImportError,self.__dict__.get('_ppimport_exc_info')[1]")
                contents = contents[:pos] + contents[pos + length + 1:]

            if "[k]*step+start)" in contents:
                pos = contents.find("[k]*step+start)")
                contents = contents[:pos + 17] + contents[pos + 21:]
            badstring = ["silly_field", "('id', models.AutoField(primary_key=True))"]

            while "check_framework.Model2." in contents:
                pos = contents.find("check_framework.Model2.")
                area = contents[pos - 300:pos + 300]
                start = area.find("class")
                end = area.find("def")
                contents = contents[:pos - 300 + start] + contents[pos - 300 + end:]

            fromhere = 0
            while "DEFAULT_KMS_KEY_NAME" in contents[fromhere:] and "ENCRYPTION_CONFIG" in contents[
                                                                                           fromhere:fromhere + 2000]:
                pos = fromhere + contents[fromhere:].find("DEFAULT_KMS_KEY_NAME")
                area = contents[pos - 1000:pos + 1000]
                start = area[:1000].find("class")
                if (start == -1):
                    start = area[:1000].find("from")
                if (start == -1):
                    start = area[:1000].find("import")
                if (start == -1):
                    start = area[:1000].find("def")

                end = area[1000:].find("def")
                if (end == -1):
                    end = area[1000:].find("from")
                if (end == -1):
                    end = area[1000:].find("import")

                print("Found it at  " + str(pos))
                #    print(len(contents))
                if (start > 0 and end > 0):
                    contents = contents[:pos - 1000 + start] + contents[pos - 1000 + end:]
                    fromhere = pos - 1000 + start + end + 1
                    print("countinue at " + str(fromhere))
                    print(start)
                    print(end)
                else:
                    fromhere = pos + 1000

            fromhere = 0
            while "somepassword" in contents[fromhere:]:
                pos = fromhere + contents[fromhere:].find("somepassword")
                area = contents[pos - 1000:pos + 1000]
                start = area.find("def")
                end = area[1000:].find("def")
                if (end == -1):
                    end = area[1000:].find("from")
                if (end == -1):
                    end = area[1000:].find("import")
                if start > 0 and end > 0:
                    contents = contents[:pos - 1000 + start] + contents[pos + end:]
                    fromhere = pos - 1000 + start

                else:
                    fromhere = pos + 1

            if "somepassword" in contents and "someuser" in contents and "somehost" in contents:
                pos = contents.find("somepassword")

            for x in badstring:
                while x in contents:
                    pos = contents.find(x)
                    area = contents[pos - 500:pos + 1000]
                    if "db.create_table" in area:
                        contents = contents.replace("('id', models.AutoField(primary_key=True))",
                                                    "('id', models.AutoField(primary_key=False))", 1)
                        continue
                    start = area.find("class")
                    rest_area = area[start:]
                    end = rest_area.find("from") + start
                    end2 = rest_area.find("import") + start
                    if end2 < end:
                        end = end2
                    if (end > start):
                        contents = contents[:pos - 500 + start] + contents[pos - 500 + end:]

        with open("w2v/pythontraining.txt", "w", encoding="utf8") as clean_python_train_data:
            clean_python_train_data.write(contents)

    def merge_corpus(self):
        fulltext = ""
        for i in range(0, self.part):
            with open(f"w2v/pythontraining_{self.mode}_{str(i)}", "r", encoding="utf8") as file:
                contents = file.read()
                fulltext = f"{fulltext}{contents}"
                print("loaded " + str(i))

        with open(f'w2v/pythontraining_{self.mode}_X', 'w', encoding="utf8") as outfile:
            outfile.write(fulltext)


tokenizer = W2VPythonTokenize()
tokenizer.collect_data_for_build_python_dictionary_corpus()
tokenizer.default_tokenizer()
tokenizer.python_tokenize()
tokenizer.merge_corpus()
