import io
import subprocess

# default python tokenizer https://bit.ly/3QNJsKf
p = subprocess.Popen(["python", "-m", "tokenize", "w2v/pythontraining.txt"], stdout=subprocess.PIPE)
out, err = p.communicate()

tokenizer_result = io.StringIO(out)


class PythonTokenize:
    def __init__(self):
        self.python_data = ""
        self.mode = "withString"
        self.count = 0
        self.total_count = 0
        self.comment = 0
        self.part = 0

    def python_tokenize(self):
        for line in tokenizer_result:
            self.total_count += 1
            self.count += 1
            if self.total_count % 1000 == 0:
                print(self.total_count)
            position1 = line.find(":") + 1
            position2 = line.find("'")
            position3 = line[position2 + 1:].find("'")

            cat = line[position1:position2]
            content = line[position2 + 1:-2]

            if ('"""' in line) or ("COMMENT" in cat):
                self.comment += 1
                continue

            if ("NL" in cat) or ("NEWLINE" in cat):
                self.python_data = f"{self.python_data}\n"
            elif "INDENT" in cat:
                for x in range(content.count('t')):
                    self.python_data = f"{self.python_data}  "
            else:
                self.python_data = f"{self.python_data} {content}"

            # split into several files to reduce the computational load
            if self.count > 1000000:
                print("saving part " + str(self.part) + " (" + self.mode + ") " + str(self.total_count))
                with open(f'w2v/pythontraining_{self.mode}_{str(self.part)}', 'w', encoding="utf8") as outfile:
                    outfile.write(self.python_data)
                self.python_data = ""
                self.part += 1
                self.count = 0

        with open(f'w2v/pythontraining_{self.mode}_{str(self.part)}', 'w', encoding="utf8") as outfile:
            outfile.write(self.python_data)

    def merge_corpus(self):
        fulltext = ""
        for i in range(0, self.part):
            with open(f"w2v/pythontraining_{self.mode}_{str(i)}", "r", encoding="utf8") as file:
                contents = file.read()
                fulltext = f"{fulltext}{contents}"
                print("loaded " + str(i))

        with open(f'w2v/pythontraining_{self.mode}_X', 'w', encoding="utf8") as outfile:
            outfile.write(fulltext)


tokenizer = PythonTokenize()
tokenizer.python_tokenize()
tokenizer.merge_corpus()
