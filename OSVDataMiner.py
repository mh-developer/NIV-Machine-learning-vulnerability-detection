import json
import os
import sys
import time

import requests


class OSVDataMiner:
    def __init__(self):
        if not os.path.isfile('github_access_token.txt'):
            print("please place a Github access token in 'github_access_token.txt' directory.")
            sys.exit()

        with open('github_access_token.txt', 'r') as access_token:
            self.access = access_token.readline().replace("\n", "")

        self.headers = {'Accept': 'application/vnd.github+json', 'Authorization': f'token {self.access}'}

    def getting_data(self):
        commits = {}
        if os.path.isfile('all_commits.json'):
            with open('all_commits.json', 'r') as infile:
                commits = json.load(infile)

        parameters = [
            "buffer overflow", "denial of service", "dos", "XXE", "vuln", "CVE", "XSS", "NVD", "malicious",
            "cross site", "exploit", "directory traversal", "rce", "remote code execution", "XSRF",
            "cross site request forgery", "click jack", "clickjack", "session fixation", "cross origin",
            "infinite loop", "brute force", "buffer overflow", "cache overflow", "command injection",
            "cross frame scripting", "csv injection", "eval injection", "execution after redirect",
            "format string", "path disclosure", "function injection", "replay attack", "session hijacking", "smurf",
            "sql injection", "flooding", "tampering", "sanitize", "sanitise", "unauthorized", "unauthorised"
        ]

        prefixes = [
            "prevent", "fix", "attack", "protect", "issue", "correct", "update", "improve", "change", "check",
            "malicious", "insecure", "vulnerable", "vulnerability"
        ]

        for k in parameters:
            for pre in prefixes:
                self.find_commits(k + " " + pre, commits)

    def find_commits(self, key, commits):
        maximum = 3000
        new = 0

        params = (
            ('q', key), ('per_page', 100)
        )
        next_link = "https://api.github.com/search/commits"

        for i in range(0, maximum):
            print(str(len(commits)) + " commits so far.")

            limit = 0
            while limit == 0:
                response = requests.get(next_link, headers=self.headers, params=params)
                h = response.headers
                if 'X-RateLimit-Remaining' in h:
                    limit = int(h['X-RateLimit-Remaining'])
                    if limit == 0:
                        print("Rate limit. Sleep.")
                        time.sleep(35)
            if 'Link' not in h:
                break

            content = response.json()
            for k in range(0, len(content["items"])):
                repo = content["items"][k]["repository"]["html_url"]
                if repo not in commits:
                    c = {
                        "url": content["items"][k]["url"],
                        "html_url": content["items"][k]["html_url"],
                        "message": content["items"][k]["commit"]["message"],
                        "sha": content["items"][k]["sha"],
                        "keyword": key
                    }
                    commits[repo] = {}
                    commits[repo][content["items"][k]["sha"]] = c
                else:
                    if not content["items"][k]["sha"] in commits[repo]:
                        new = new + 1
                        c = {
                            "url": content["items"][k]["url"],
                            "html_url": content["items"][k]["html_url"],
                            "sha": content["items"][k]["sha"],
                            "keyword": key
                        }
                        commits[repo][content["items"][k]["sha"]] = c

            link = h['Link']
            ref_links = self.analyze_links(link)
            if "last" in ref_links:
                last_number = ref_links["last"].split("&page=")[1]
                maximum = int(last_number) - 1
            if not "next" in ref_links:
                break
            else:
                next_link = ref_links["next"]

        with open('all_commits.json', 'w') as outfile:
            json.dump(commits, outfile)

    def find_pypi_commits(self):
        osv_pypi_packages = "https://api.github.com/repos/pypa/advisory-database/contents/vulns"
        osv_vulnerability_url = 'https://api.osv.dev/v1/vulns'
        progress_time_start = time.time()

        response = requests.get(osv_pypi_packages, headers=self.headers)
        folders: list = response.json() if response and response.status_code == 200 else []
        osv_data = {}

        if os.path.isfile('osv_packages.json'):
            with open('osv_packages.json', 'r') as infile:
                osv_data = json.load(infile)

        progress = 0
        for content in folders:
            folder = content['name']
            file_type = content['type']
            progress += 1
            if progress % 10 == 0:
                print(f"Progress: {progress}/{len(folders)} ({(progress / len(folders)) * 100}%)")

            if file_type == 'dir' and folder not in osv_data:
                dir_files = requests.get(f'{osv_pypi_packages}/{folder}', headers=self.headers)
                if 'X-RateLimit-Remaining' in response.headers:
                    limit = int(response.headers['X-RateLimit-Remaining'])
                    if limit == 0:
                        print("Rate limit. Sleep.")
                        time.sleep(60)

                for dir_file in dir_files.json():
                    if dir_file['type'] == 'file':
                        if folder not in osv_data:
                            osv_package = {
                                "osv_id": dir_file['name'][:-5],
                                "package": content['path'].split('/')[1]
                            }
                            osv_data[folder] = {}
                            osv_data[folder][dir_file['name'][:-5]] = osv_package
                        elif dir_file['name'][:-5] not in osv_data[folder]:
                            osv_package = {
                                "osv_id": dir_file['name'][:-5],
                                "package": content['path'].split('/')[1]
                            }
                            osv_data[folder][dir_file['name'][:-5]] = osv_package

                with open('osv_packages.json', 'w') as outfile:
                    json.dump(osv_data, outfile)

        progress_time_middle = time.time()
        print("Number of packages:", len(osv_data))
        print("Number of documented vulnerabilities:", len(osv_data.values()))
        print(f'Time to process PyPI osv data: {progress_time_middle - progress_time_start}')

        osv_vulnerability_details = {}
        for osv_vulnerability_package in osv_data.keys():
            for osv_vulnerability_id in osv_data[osv_vulnerability_package].keys():
                osv_vulnerability_data = requests.get(f'{osv_vulnerability_url}/{osv_vulnerability_id}')
                osv_vulnerability_details[f'{osv_vulnerability_package}|{osv_vulnerability_id}'] = osv_vulnerability_data.json()

        with open('osv_vulnerability_details.json', 'w') as outfile:
            json.dump(osv_vulnerability_details, outfile)

        progress_time_end = time.time()
        print('processing end.')
        print(f'Time to process details: {progress_time_end - progress_time_middle}')
        print(f'Time to process all: {progress_time_end - progress_time_start}')

    def analyze_links(self, link):
        link = link + ","
        ref_links = {}
        while "," in link:
            pos = link.find(",")
            text = link[:pos]
            rest = link[pos + 1:]
            try:
                if "\"next\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["next"] = text
                if "\"prev\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["prev"] = text
                if "\"first\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["first"] = text
                if "\"last\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["last"] = text
            except IndexError as e:
                print(e)
                print("\n")
                print(text)
                print("\n\n")
                sys.exit()
            link = rest
        return ref_links


miner = OSVDataMiner()
miner.find_pypi_commits()
