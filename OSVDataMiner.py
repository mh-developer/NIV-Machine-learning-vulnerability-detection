import json
import os
import sys
import time

import requests


class OSVDataMiner:
    def __init__(self):
        pass

    def getting_data(self):

        if not os.path.isfile('github_access_token.txt'):
            print("please place a Github access token in this directory.")
            sys.exit()

        with open('github_access_token.txt', 'r') as access_token:
            access = access_token.readline().replace("\n", "")

        commits = {}
        if os.path.isfile('all_commits.json'):
            with open('all_commits.json', 'r') as infile:
                commits = json.load(infile)

        parameters = [
            "buffer overflow", "denial of service", "dos", "XXE", "vuln", "CVE", "XSS", "NVD", "malicious",
            "cross site", "exploit", "directory traversal", "rce", "remote code execution", "XSRF",
            "cross site request forgery", "click jack", "clickjack", "session fixation", "cross origin",
            "infinite loop", "brute force", "buffer overflow", "cache overflow", "command injection",
            "cross frame scripting", "csv injection", "eval injection", "execution after redirect",
            "format string", "path disclosure", "function injection", "replay attack", "session hijacking", "smurf",
            "sql injection", "flooding", "tampering", "sanitize", "sanitise", "unauthorized", "unauthorised"
        ]

        prefixes = [
            "prevent", "fix", "attack", "protect", "issue", "correct", "update", "improve", "change", "check",
            "malicious", "insecure", "vulnerable", "vulnerability"
        ]

        for k in parameters:
            for pre in prefixes:
                self.find_commits(k + " " + pre, commits, access)

    def find_commits(self, key, commits, access):
        maximum = 3000
        new = 0

        params = (
            ('q', key), ('per_page', 100)
        )
        myheaders = {'Accept': 'application/vnd.github.cloak-preview', 'Authorization': 'token ' + access}
        next_link = "https://api.github.com/search/commits"

        for i in range(0, maximum):
            print(str(len(commits)) + " commits so far.")

            limit = 0
            while limit == 0:
                response = requests.get(next_link, headers=myheaders, params=params)
                h = response.headers
                if 'X-RateLimit-Remaining' in h:
                    limit = int(h['X-RateLimit-Remaining'])
                    if limit == 0:
                        print("Rate limit. Sleep.")
                        time.sleep(35)
            if 'Link' not in h:
                break

            content = response.json()
            for k in range(0, len(content["items"])):
                repo = content["items"][k]["repository"]["html_url"]
                if repo not in commits:
                    c = {
                        "url": content["items"][k]["url"],
                        "html_url": content["items"][k]["html_url"],
                        "message": content["items"][k]["commit"]["message"],
                        "sha": content["items"][k]["sha"],
                        "keyword": key
                    }
                    commits[repo] = {}
                    commits[repo][content["items"][k]["sha"]] = c
                else:
                    if not content["items"][k]["sha"] in commits[repo]:
                        # new commit for this already known repository
                        new = new + 1
                        c = {
                            "url": content["items"][k]["url"],
                            "html_url": content["items"][k]["html_url"],
                            "sha": content["items"][k]["sha"],
                            "keyword": key
                        }
                        commits[repo][content["items"][k]["sha"]] = c

            link = h['Link']
            ref_links = self.analyze_links(link)
            if "last" in ref_links:
                last_number = ref_links["last"].split("&page=")[1]
                maximum = int(last_number) - 1
            if not "next" in ref_links:
                break
            else:
                next_link = ref_links["next"]

        with open('all_commits.json', 'w') as outfile:
            json.dump(commits, outfile)

    def analyze_links(self, link):
        link = link + ","
        ref_links = {}
        while "," in link:
            pos = link.find(",")
            text = link[:pos]
            rest = link[pos + 1:]
            try:
                if "\"next\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["next"] = text
                if "\"prev\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["prev"] = text
                if "\"first\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["first"] = text
                if "\"last\"" in text:
                    text = text.split("<")[1]
                    text = text.split(">;")[0]
                    ref_links["last"] = text
            except IndexError as e:
                print(e)
                print("\n")
                print(text)
                print("\n\n")
                sys.exit()
            link = rest
        return ref_links


miner = OSVDataMiner()
miner.getting_data()
