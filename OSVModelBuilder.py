import json
import os.path
import pickle
import random
import sys
from datetime import datetime

import numpy
from gensim.models import Word2Vec
from keras.layers import Dense, LSTM, GRU, BatchNormalization
from keras.models import Sequential
from keras.preprocessing import sequence
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils import class_weight

import helper


class OSVModelBuilder:
    def __init__(self):
        self.mode = "sql"

        self.restriction = [20000, 5, 6, 10]
        self.step = 5
        self.full_length = 200
        self.mode2 = f'{str(self.step)}_{str(self.full_length)}'

        self.min_count = 10
        self.iteration = 300
        self.s = 200
        self.w = "withString"

        self.w2v = f"word2vec_{self.w}{str(self.min_count)}-{str(self.iteration)}-{str(self.s)}"
        self.w2vmodel = f"w2v/{self.w2v}.model"
        self.w2v_model = None
        self.word_vectors = None

        self.all_blocks = []

        self.train_X = []
        self.train_Y = []
        self.validate_X = []
        self.validate_Y = []
        self.final_test_X = []
        self.final_test_Y = []

    def load_data(self):
        if len(sys.argv) > 1:
            self.mode = sys.argv[1]

        if not (os.path.isfile(self.w2vmodel)):
            print("word2vec model is still being created...")
            sys.exit()

        self.w2v_model = Word2Vec.load(self.w2vmodel)
        self.word_vectors = self.w2v_model.wv

        # with open(self.w2vmodel, 'rb') as model_file:
        #     self.w2v_model = pickle.load(model_file)
        # self.w2v_model = numpy.load(self.w2vmodel, allow_pickle=True)

        # w = KeyedVectors.load_word2vec_format(self.w2vmodel, binary=True)
        # self.w2v_model = KeyedVectors.load_word2vec_format(self.w2vmodel, binary=True)

        with open(f'data/plain_{self.mode}', 'r') as infile:
            data = json.load(infile)

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("finished loading. ", now_format)

        progress = 0
        count = 0
        for r in data:
            progress += 1

            for c in data[r]:

                if "files" in data[r][c]:
                    #  if len(data[r][c]["files"]) > self.restriction[3]:
                    #    continue

                    for f in data[r][c]["files"]:

                        #      if len(data[r][c]["files"][f]["changes"]) >= self.restriction[2]:
                        #         continue

                        if "source" not in data[r][c]["files"][f]:
                            continue

                        if "source" in data[r][c]["files"][f]:
                            sourcecode = data[r][c]["files"][f]["source"]
                            #     if len(sourcecode) > self.restriction[0]:
                            #       continue

                            all_bad_parts = []

                            for change in data[r][c]["files"][f]["changes"]:
                                bad_parts = change["badparts"]
                                count += len(bad_parts)

                                #     if len(badparts) > self.restriction[1]:
                                #       break

                                for bad in bad_parts:
                                    pos = helper.find_position(bad, sourcecode)
                                    if not -1 in pos:
                                        all_bad_parts.append(bad)

                            #   if (len(allbadparts) > self.restriction[2]):
                            #     break

                            if len(all_bad_parts) > 0:
                                #   if len(allbadparts) < self.restriction[2]:
                                positions = helper.find_positions(all_bad_parts, sourcecode)
                                blocks = helper.get_blocks(sourcecode, positions, self.step, self.full_length)

                                for b in blocks:
                                    self.all_blocks.append(b)

    def prepare_learning_dataset(self):
        keys = []

        for i in range(len(self.all_blocks)):
            keys.append(i)
        random.shuffle(keys)

        cutoff = round(0.7 * len(keys))
        cutoff2 = round(0.85 * len(keys))

        keys_train = keys[:cutoff]
        keys_test = keys[cutoff:cutoff2]
        keys_final_test = keys[cutoff2:]

        print("cutoff " + str(cutoff))
        print("cutoff2 " + str(cutoff2))

        with open(f'data/{self.mode}_dataset_keystrain', 'wb') as fp:
            pickle.dump(keys_train, fp)
        with open(f'data/{self.mode}_dataset_keystest', 'wb') as fp:
            pickle.dump(keys_test, fp)
        with open(f'data/{self.mode}_dataset_keysfinaltest', 'wb') as fp:
            pickle.dump(keys_final_test, fp)

        print(f"Creating training dataset... ({self.mode})")
        for k in keys_train:
            block = self.all_blocks[k]
            code = block[0]
            token = helper.get_tokens(code)
            vectorlist = []
            for t in token:
                if t in self.word_vectors.vocab and t != " ":
                    vector = self.w2v_model[t]
                    vectorlist.append(vector.tolist())
            self.train_X.append(vectorlist)
            self.train_Y.append(block[1])

        print("Creating validation dataset...")
        for k in keys_test:
            block = self.all_blocks[k]
            code = block[0]
            token = helper.get_tokens(code)
            vectorlist = []
            for t in token:
                if t in self.word_vectors.vocab and t != " ":
                    vector = self.w2v_model[t]
                    vectorlist.append(vector.tolist())
            self.validate_X.append(vectorlist)
            self.validate_Y.append(block[1])

        print("Creating finaltest dataset...")
        for k in keys_final_test:
            block = self.all_blocks[k]
            code = block[0]
            token = helper.get_tokens(code)
            vectorlist = []
            for t in token:
                if t in self.word_vectors.vocab and t != " ":
                    vector = self.w2v_model[t]
                    vectorlist.append(vector.tolist())
            self.final_test_X.append(vectorlist)
            self.final_test_Y.append(block[1])

        print("Train length: " + str(len(self.train_X)))
        print("Test length: " + str(len(self.validate_X)))
        print("Finaltesting length: " + str(len(self.final_test_X)))
        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("time: ", now_format)

        with open(f'data/plain_{self.mode}_dataset-train-X_{self.w2v}__{self.mode2}', 'wb') as fp:
            pickle.dump(self.train_X, fp)
        with open(f'data/plain_{self.mode}_dataset-train-Y_{self.w2v}__{self.mode2}', 'wb') as fp:
            pickle.dump(self.train_Y, fp)
        with open(f'data/plain_{self.mode}_dataset-validate-X_{self.w2v}__{self.mode2}', 'wb') as fp:
            pickle.dump(self.validate_X, fp)
        with open(f'data/plain_{self.mode}_dataset-validate-Y_{self.w2v}__{self.mode2}', 'wb') as fp:
            pickle.dump(self.validate_Y, fp)
        with open(f'data/{self.mode}_dataset_finaltest_X', 'wb') as fp:
            pickle.dump(self.final_test_X, fp)
        with open(f'data/{self.mode}_dataset_finaltest_Y', 'wb') as fp:
            pickle.dump(self.final_test_Y, fp)
        print("saved finaltest.")

    def processing_LSTM_and_GRU_model(self):
        X_train = numpy.array(self.train_X)
        y_train = numpy.array(self.train_Y)
        X_test = numpy.array(self.validate_X)
        y_test = numpy.array(self.validate_Y)
        X_finaltest = numpy.array(self.final_test_X)
        y_finaltest = numpy.array(self.final_test_Y)

        for i in range(len(y_train)):
            y_train[i] = 1 if y_train[i] == 0 else 0

        for i in range(len(y_test)):
            y_test[i] = 1 if y_test[i] == 0 else 0

        for i in range(len(y_finaltest)):
            y_finaltest[i] = 1 if y_finaltest[i] == 0 else 0

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("numpy array done. ", now_format)

        print(str(len(X_train)) + " samples in the training set.")
        print(str(len(X_test)) + " samples in the validation set.")
        print(str(len(X_finaltest)) + " samples in the final test set.")

        csum = sum(y_train)
        print("percentage of vulnerable samples: " + str(int((csum / len(X_train)) * 10000) / 100) + "%")

        test_vul = sum(1 for y in y_test if y == 1)
        print("absolute amount of vulnerable samples in test set: " + str(test_vul))

        max_length = self.full_length

        # parameters
        dropout = 0.2
        neurons = 100
        optimizer = "adam"
        epochs = 100
        batch_size = 128

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("Starting LSTM: ", now_format)

        print("Dropout: " + str(dropout))
        print("Neurons: " + str(neurons))
        print("Optimizer: " + optimizer)
        print("Epochs: " + str(epochs))
        print("Batch Size: " + str(batch_size))
        print("max length: " + str(max_length))

        X_train = sequence.pad_sequences(X_train, maxlen=max_length)
        X_test = sequence.pad_sequences(X_test, maxlen=max_length)
        X_finaltest = sequence.pad_sequences(X_finaltest, maxlen=max_length)

        # creating LSTM model
        model_LSTM = Sequential()
        model_LSTM.add(LSTM(neurons, dropout=dropout, recurrent_dropout=dropout))
        model_LSTM.add(Dense(1, activation='sigmoid'))
        model_LSTM.compile(loss=helper.f1_loss, optimizer='adam', metrics=[helper.f1])

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("Compiled LSTM: ", now_format)

        class_weights_LSTM = class_weight.compute_class_weight('balanced', numpy.unique(y_train), y_train)

        history_LSTM = model_LSTM.fit(
            X_train,
            y_train,
            epochs=epochs,
            batch_size=batch_size,
            class_weight=class_weights_LSTM
        )

        # creating GRU model
        model_GRU = Sequential()
        model_GRU.add(GRU(neurons, dropout=dropout, recurrent_dropout=dropout))
        model_GRU.add(BatchNormalization())
        model_GRU.add(Dense(1, activation='sigmoid'))
        print(model_GRU.summary())

        model_GRU.compile(
            loss=helper.f1_loss,
            optimizer="sgd",
            metrics=[helper.f1],
        )

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print("Compiled GRU: ", now_format)

        class_weights_GRU = class_weight.compute_class_weight('balanced', numpy.unique(y_train), y_train)

        history_GRU = model_GRU.fit(
            X_train,
            y_train,
            epochs=epochs,
            batch_size=batch_size,
            class_weight=class_weights_GRU
        )

        # evaluation data
        for dataset in ["train", "test", "finaltest"]:
            print(f"Now predicting on {dataset} set ({str(dropout)} dropout)")

            if dataset == "train":
                yhat_classes_LSTM = model_LSTM.predict_classes(X_train, verbose=0)
                yhat_classes_GRU = model_GRU.predict_classes(X_train, verbose=0)
                accuracy_LSTM = accuracy_score(y_train, yhat_classes_LSTM)
                precision_LSTM = precision_score(y_train, yhat_classes_LSTM)
                recall_LSTM = recall_score(y_train, yhat_classes_LSTM)
                F1Score_LSTM = f1_score(y_train, yhat_classes_LSTM)
                accuracy_GRU = accuracy_score(y_train, yhat_classes_GRU)
                precision_GRU = precision_score(y_train, yhat_classes_GRU)
                recall_GRU = recall_score(y_train, yhat_classes_GRU)
                F1Score_GRU = f1_score(y_train, yhat_classes_GRU)

            if dataset == "test":
                yhat_classes_LSTM = model_LSTM.predict_classes(X_test, verbose=0)
                yhat_classes_GRU = model_GRU.predict_classes(X_test, verbose=0)
                accuracy_LSTM = accuracy_score(y_test, yhat_classes_LSTM)
                precision_LSTM = precision_score(y_test, yhat_classes_LSTM)
                recall_LSTM = recall_score(y_test, yhat_classes_LSTM)
                F1Score_LSTM = f1_score(y_test, yhat_classes_LSTM)
                accuracy_GRU = accuracy_score(y_test, yhat_classes_GRU)
                precision_GRU = precision_score(y_test, yhat_classes_GRU)
                recall_GRU = recall_score(y_test, yhat_classes_GRU)
                F1Score_GRU = f1_score(y_test, yhat_classes_GRU)

            if dataset == "finaltest":
                yhat_classes_LSTM = model_LSTM.predict_classes(X_finaltest, verbose=0)
                yhat_classes_GRU = model_GRU.predict_classes(X_finaltest, verbose=0)
                accuracy_LSTM = accuracy_score(y_finaltest, yhat_classes_LSTM)
                precision_LSTM = precision_score(y_finaltest, yhat_classes_LSTM)
                recall_LSTM = recall_score(y_finaltest, yhat_classes_LSTM)
                F1Score_LSTM = f1_score(y_finaltest, yhat_classes_LSTM)
                accuracy_GRU = accuracy_score(y_finaltest, yhat_classes_GRU)
                precision_GRU = precision_score(y_finaltest, yhat_classes_GRU)
                recall_GRU = recall_score(y_finaltest, yhat_classes_GRU)
                F1Score_GRU = f1_score(y_finaltest, yhat_classes_GRU)

            print("Accuracy LSTM: " + str(accuracy_LSTM))
            print("Precision LSTM: " + str(precision_LSTM))
            print("Recall LSTM: " + str(recall_LSTM))
            print('F1 score LSTM: %f' % F1Score_LSTM)
            print("Accuracy GRU: " + str(accuracy_GRU))
            print("Precision GRU: " + str(precision_GRU))
            print("Recall GRU: " + str(recall_GRU))
            print('F1 score GRU: %f' % F1Score_GRU)
            print("\n")

        now = datetime.now()
        now_format = now.strftime("%H:%M")
        print(f"saving LSTM model {self.mode}. {now_format}")
        model_LSTM.save(f'model/LSTM_model_{self.mode}.h5')
        model_GRU.save(f'model/GRU_model_{self.mode}.h5')
        print("\n\n")


builder = OSVModelBuilder()
builder.load_data()
builder.prepare_learning_dataset()
builder.processing_LSTM_and_GRU_model()
